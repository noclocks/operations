# Generative Pre-trained Transformer (GPT)

Generative Pre-Trained Transformer (GPT) is a *transformer-based model* that is trained on a large corpus of text data to generate human-like text.

It is a type of Large Language Model (LLM) that can generate text by predicting the next word in a sequence of words.

GPT is a powerful tool for generating text that is coherent and contextually relevant.

## Overview

GPT's architecture is based on the transformer model, which is a type of neural network that is designed to process sequential data.

The concept became popularize primarily through the release of the `GPT-*` models released by `OpenAI` since 2019 (`GPT-2`, `GPT-3`, `GPT-4`).

Some details about GPT:
